{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) Overview\n",
    "\n",
    "According to the paper on [arxiv.org](https://arxiv.org/abs/2010.11929), ViT functions as follows:\n",
    "\n",
    "1. **Image Patching**: The input image is divided into patches of a fixed size.\n",
    "2. **Patch Embedding Calculation**: Each patch is converted into a corresponding embedding.\n",
    "3. **Adding Position Embeddings and Class Token**: Position embeddings and a class token are appended to each patch embedding.\n",
    "4. **Transformer Encoder**: The sequence of embeddings is then input into a Transformer encoder.\n",
    "5. **MLP Head for Classification**: Finally, the resulting representations are passed through a Multi-Layer Perceptron (MLP) head to obtain the class predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model \n",
    "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH EXTRACTION\n",
    "\n",
    "class PatchExtractor(Layer):\n",
    "    def __init__(self):\n",
    "        super(PatchExtractor, self).__init__()\n",
    "\n",
    "    def call(self,images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches( \n",
    "        images = images,\n",
    "        sizes = [1, 16, 16, 1],\n",
    "        strides = [1, 16, 16 ,1],\n",
    "        rates = [1, 1, 1, 1],\n",
    "        padding = \"VALID\")\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "        \n",
    "image = plt.imread('cat.jpg')\n",
    "image = tf.image.resize(tf.convert_to_tensor(image), size=(224, 224))\n",
    "plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (224 x 224) we get 196 patches of (16 x 16)\n",
    "\n",
    "batch = tf.expand_dims(image, axis=0)\n",
    "patches = PatchExtractor()(batch)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISING THE PATCHES WE EXTRACTED\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    # print()\n",
    "    ax = plt.subplot(n, n, i+1)\n",
    "    patch_img = tf.reshape(patch, (16, 16, 3))\n",
    "    ax.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATCH ENCODING\n",
    "\n",
    "# Patch Encoder Process\n",
    "\n",
    "## Overview\n",
    "The patch encoder is a crucial component in vision transformer models. It processes image patches to create embeddings that can be fed into a transformer architecture.\n",
    "\n",
    "## Process Steps\n",
    "\n",
    "1. **Input**\n",
    "   - The encoder receives image patches as input.\n",
    "   - These patches are uniform, non-overlapping sections of the original image.\n",
    "\n",
    "2. **Patch Embeddings**\n",
    "   - Each patch is flattened into a 1D vector.\n",
    "   - This vector passes through a linear projection (typically a fully connected layer).\n",
    "   - The output is the patch embedding, representing the content of the patch.\n",
    "\n",
    "3. **Positional Embeddings**\n",
    "   - For each patch, a positional embedding is created.\n",
    "   - This embedding encodes the patch's position in the original image.\n",
    "   - It's usually a learned vector with the same dimensions as the patch embedding.\n",
    "\n",
    "4. **Combination**\n",
    "   - The patch embedding and its corresponding positional embedding are added together.\n",
    "   - This combination preserves both content and spatial information.\n",
    "\n",
    "5. **Output**\n",
    "   - The result is a sequence of embedded patches.\n",
    "   - Each embedded patch contains both content and position information.\n",
    "   - This sequence is ready for input into a Transformer model.\n",
    "\n",
    "## Significance\n",
    "This process allows the Transformer to treat the image as a sequence of tokens, similar to how it processes text. The inclusion of positional information helps the model understand spatial relationships between different parts of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a trainable that will learn the [class] token embeddings\n",
    "\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches=196, projection_dim=768):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        class_token = w_init(shape=(1, projection_dim), dtype=\"float32\")\n",
    "        self.class_token = tf.Variable(initial_value=class_token, trainable=True)\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = Embedding(input_dim=num_patches+1, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        batch = tf.shape(patch)[0]\n",
    "        class_token = tf.tile(self.class_token, multiples=[batch,1])\n",
    "        class_token = tf.reshape(class_token, (batch, 1, self.projection_dim))\n",
    "\n",
    "        patches_embedd = self.projection(patch)\n",
    "        patches_embedd = tf.concat([patches_embedd, class_token],1)\n",
    "\n",
    "        positions = tf.range(start=0, limit=self.num_patches+1, delta=1)\n",
    "        positions_embedd = self.position_embedding(positions)\n",
    "\n",
    "        encoded = patches_embedd + positions_embedd\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = PatchEncoder()(patches)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTILAYER PERCEPTRON\n",
    "\n",
    "#  It is used in the Transformer encoder as well as the final output layer of the ViT mode.\n",
    "\n",
    "class MLP(Layer):\n",
    "    def __init__(self, hidden_features, out_features, dropout_rate=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = Dense(hidden_features, activation=tf.nn.gelu)\n",
    "        self.dense2 = Dense(out_features)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        y = self.dropout(x)\n",
    "        return y\n",
    "\n",
    "mlp = MLP(768* 2, 768)\n",
    "y = mlp(tf.zeros((1, 197, 768)))\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTING THE TRANSFORMER ENCODER\n",
    "\n",
    "class Block(Layer):\n",
    "    def __init__(self, projection_dim, num_heads=4, dropout_rate=0.1):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = MLP(projection_dim * 2, projection_dim, dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x1 = self.norm1(x)\n",
    "        attn_output = self.attn(x1,x1)\n",
    "        x2 = Add()([attn_output, x])\n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3)\n",
    "\n",
    "        y = Add()([x3, x2])\n",
    "        return y\n",
    "\n",
    "block = Block(768)\n",
    "y = block(tf.zeros((1, 197, 768)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, projection_dim, num_heads=4, num_blocks=12, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.blocks = [Block(projection_dim, num_heads, dropout_rate) for _ in range(num_blocks)]\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Create a [batch_size, projection_dim] tensor.\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        y = self.dropout(x)\n",
    "        return y\n",
    "transformer = TransformerEncoder(768)\n",
    "y = transformer(embeddings)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VisionTransformer(num_classes, num_patches=196, projection_dim=768, input_shape=(224, 224, 3)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Patch extractor\n",
    "    patches = PatchExtractor()(inputs)\n",
    "    # Patch encoder\n",
    "    patches_embed = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    # Transformer encoder\n",
    "    representation = TransformerEncoder(projection_dim)(patches_embed)\n",
    "    representation = GlobalAveragePooling1D()(representation)\n",
    "    # MLP to classify outputs\n",
    "    logits = MLP(projection_dim, num_classes, 0.5)(representation)\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "model = create_VisionTransformer(2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
